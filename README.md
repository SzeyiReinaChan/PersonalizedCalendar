# cs5100_spring24_project
Project question:
Decision-making with the goal of max ğ‘Ÿ without knowing ğ‘Ÿ a-priori by requiring S to learn about r using human feedback.
-compare 2 types of human feedback: (1) direct reward feedback[1]; (2) corrective feedback[2]

Assumption: reward is linearly parameterized by a human preference vector Î¸ and a binary feature vector describing the calendar[3].

Problem formulation:
1. A fixed set of relevant events ğ¸_ğ‘Ÿ, irrelevant events ğ¸_ğ‘–ğ‘Ÿ.
2. A calendar ğ¶=K slots, e.g., ğ¶=[âˆ…,âˆ…,ğ‘’_1,ğ‘’_2,ğ‘’_1,âˆ…].
3. For each episode t=1,â€¦,T. S observes the current state ğ‘ _ğ‘¡ and proposes an action ğ‘_ğ‘¡.
4. The action incurs reward ğ‘Ÿ(s_ğ‘¡,ğ‘_ğ‘¡ ). (No state transition, simplifying MDP to contextual bandit.)

[1] Li, Lihong, et al. "A contextual-bandit approach to personalized news article recommendation." Proceedings of the 19th international conference on World wide web. 2010.
[2] Shivaswamy, Pannaga, and Thorsten Joachims. "Coactive learning." Journal of Artificial Intelligence Research 53 (2015): 1-40.
[3] Gervasio, Melinda T., et al. "Active preference learning for personalized calendar scheduling assistance." Proceedings of the 10th international conference on Intelligent user interfaces. 2005.